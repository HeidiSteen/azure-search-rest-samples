@searchUrl = PUT-YOUR-SEARCH-SERVICE-ENDPOINT-HERE
@token = PUT-YOUR-PERSONAL-IDENTITY-ACCESS-TOKEN-HERE
@storageConnection = PUT-YOUR-STORAGE-CONNECTION-STRING-HERE
@imageProjectionContainer=sustainable-ai-pdf-images
@chatCompletionModelUri = PUT-YOUR-DEPLOYED-MODEL-URI-HERE
@chatCompletionModelKey = PUT-YOUR-MODEL-KEY-HERE
@textEmbeddingModelUri = PUT-YOUR-DEPLOYED-MODEL-URI-HERE
@textEmbeddingModelKey = PUT-YOUR-MODEL-KEY-HERE
@textEmbeddingDeploymentId = text-embedding-3-large
@textEmbeddingModelName = text-embedding-3-large

### Create data source 
### Source: https://github.com/Azure-Samples/azure-search-sample-data/tree/main/sustainable-ai-pdf
### Previously uploaded to Azure Blob Storage and in a container named "sustainable-ai-pdf"
POST {{searchUrl}}/datasources?api-version=2025-11-01-preview   HTTP/1.1
  Content-Type: application/json
  Authorization: Bearer {{token}}

{
   "name":"demo-multimodal-ds",
   "description":null,
   "type":"azureblob",
   "subtype":null,
   "credentials":{
      "connectionString":"{{storageConnection}}"
   },
   "container":{
      "name":"sustainable-ai-pdf",
      "query":null
   },
   "dataChangeDetectionPolicy":null,
   "dataDeletionDetectionPolicy":null,
   "encryptionKey":null,
   "identity":null
}

### Create an index
### Vectorization settings depending on your embedding model.
### For the vector field, `"dimensions":3072` is correct for text-embedding-3-large.
POST {{searchUrl}}/indexes?api-version=2025-11-01-preview   HTTP/1.1
  Content-Type: application/json
  Authorization: Bearer {{token}}

{
   "name":"demo-multimodal-index",
   "fields":[
      {
         "name":"content_id",
         "type":"Edm.String",
         "retrievable":true,
         "key":true,
         "analyzer":"keyword"
      },
      {
         "name":"text_document_id",
         "type":"Edm.String",
         "searchable":false,
         "filterable":true,
         "retrievable":true,
         "stored":true,
         "sortable":false,
         "facetable":false
      },
      {
         "name":"document_title",
         "type":"Edm.String",
         "searchable":true
      },
      {
         "name":"image_document_id",
         "type":"Edm.String",
         "filterable":true,
         "retrievable":true
      },
      {
         "name":"content_text",
         "type":"Edm.String",
         "searchable":true,
         "retrievable":true
      },
      {
         "name":"content_embedding",
         "type":"Collection(Edm.Single)",
         "dimensions":3072,
         "searchable":true,
         "retrievable":true,
         "vectorSearchProfile":"hnsw"
      },
      {
         "name":"content_path",
         "type":"Edm.String",
         "searchable":false,
         "retrievable":true
      },
      {
         "name":"offset",
         "type":"Edm.String",
         "searchable":false,
         "retrievable":true
      },
      {
         "name":"location_metadata",
         "type":"Edm.ComplexType",
         "fields":[
            {
               "name":"page_number",
               "type":"Edm.Int32",
               "searchable":false,
               "retrievable":true
            },
            {
               "name":"bounding_polygons",
               "type":"Edm.String",
               "searchable":false,
               "retrievable":true,
               "filterable":false,
               "sortable":false,
               "facetable":false
            }
         ]
      }
   ],
   "vectorSearch":{
      "profiles":[
         {
            "name":"hnsw",
            "algorithm":"defaulthnsw",
            "vectorizer":"demo-vectorizer"
         }
      ],
      "algorithms":[
         {
            "name":"defaulthnsw",
            "kind":"hnsw",
            "hnswParameters":{
               "m":4,
               "efConstruction":400,
               "metric":"cosine"
            }
         }
      ],
      "vectorizers":[
         {
            "name":"demo-vectorizer",
            "kind":"azureOpenAI",
            "azureOpenAIParameters":{
               "resourceUri": "{{textEmbeddingModelUri}}",
               "apiKey": "{{textEmbeddingModelKey}}",
               "deploymentId":"{{textEmbeddingDeploymentId}}",
               "modelName":"{{textEmbeddingModelName}}"
            }
         }
      ]
   },
   "semantic":{
      "defaultConfiguration":"semanticconfig",
      "configurations":[
         {
            "name":"semanticconfig",
            "prioritizedFields":{
               "titleField":{
                  "fieldName":"document_title"
               },
               "prioritizedContentFields":[
                  
               ],
               "prioritizedKeywordsFields":[
                  
               ]
            }
         }
      ]
   }
}

### Create a skillset
### Extraction and chunking skills: Document Extraction, Text Split
### Vectorization skills: GenAI Prompt (image verbalization), Azure OpenAI (text embedding)
POST {{searchUrl}}/skillsets?api-version=2025-11-01-preview   HTTP/1.1
  Content-Type: application/json
  Authorization: Bearer {{token}}

{
   "name":"demo-multimodal-skillset",
   "description":"A test skillset",
   "skills":[
      {
         "@odata.type":"#Microsoft.Skills.Util.DocumentExtractionSkill",
         "name":"document-extraction-skill",
         "description":"Document extraction skill to extract text and images from documents",
         "parsingMode":"default",
         "dataToExtract":"contentAndMetadata",
         "configuration":{
            "imageAction":"generateNormalizedImages",
            "normalizedImageMaxWidth":2000,
            "normalizedImageMaxHeight":2000
         },
         "context":"/document",
         "inputs":[
            {
               "name":"file_data",
               "source":"/document/file_data"
            }
         ],
         "outputs":[
            {
               "name":"content",
               "targetName":"extracted_content"
            },
            {
               "name":"normalized_images",
               "targetName":"normalized_images"
            }
         ]
      },
      {
         "@odata.type":"#Microsoft.Skills.Text.SplitSkill",
         "name":"split-skill",
         "description":"Split skill to chunk documents",
         "context":"/document",
         "defaultLanguageCode":"en",
         "textSplitMode":"pages",
         "maximumPageLength":2000,
         "pageOverlapLength":200,
         "unit":"characters",
         "inputs":[
            {
               "name":"text",
               "source":"/document/extracted_content",
               "inputs":[
                  
               ]
            }
         ],
         "outputs":[
            {
               "name":"textItems",
               "targetName":"pages"
            }
         ]
      },
      {
      "@odata.type": "#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill",
      "name": "#2",
      "context": "/document/pages/*",
      "resourceUri": "{{textEmbeddingModelUri}}",
      "apiKey": "{{textEmbeddingModelKey}}",
      "deploymentId":"{{textEmbeddingDeploymentId}}",
      "modelName":"{{textEmbeddingModelName}}",
      "dimensions": 3072,
      "inputs": [
        {
          "name": "text",
          "source": "/document/pages/*",
          "inputs": []
        }
      ],
      "outputs": [
        {
          "name": "embedding",
          "targetName": "text_vector"
        }
      ]
    },
  {
    "@odata.type": "#Microsoft.Skills.Custom.ChatCompletionSkill",
    "name": "genAI-prompt-skill",
    "description": "GenAI Prompt skill for image verbalization",
    "uri": "{{chatCompletionModelUri}}",
    "timeout": "PT1M",
    "apiKey": "{{chatCompletionModelKey}}",
    "context": "/document/normalized_images/*",
    "responseFormat": { "type": "text" },
    "inputs": [
        {
        "name": "systemMessage",
        "source": "='You are tasked with generating concise, accurate descriptions of images, figures, diagrams, or charts in documents. The goal is to capture the key information and meaning conveyed by the image without including extraneous details like style, colors, visual aesthetics, or size.\n\nInstructions:\nContent Focus: Describe the core content and relationships depicted in the image.\n\nFor diagrams, specify the main elements and how they are connected or interact.\nFor charts, highlight key data points, trends, comparisons, or conclusions.\nFor figures or technical illustrations, identify the components and their significance.\nClarity & Precision: Use concise language to ensure clarity and technical accuracy. Avoid subjective or interpretive statements.\n\nAvoid Visual Descriptors: Exclude details about:\n\nColors, shading, and visual styles.\nImage size, layout, or decorative elements.\nFonts, borders, and stylistic embellishments.\nContext: If relevant, relate the image to the broader content of the technical document or the topic it supports.\n\nExample Descriptions:\nDiagram: \"A flowchart showing the four stages of a machine learning pipeline: data collection, preprocessing, model training, and evaluation, with arrows indicating the sequential flow of tasks.\"\n\nChart: \"A bar chart comparing the performance of four algorithms on three datasets, showing that Algorithm A consistently outperforms the others on Dataset 1.\"\n\nFigure: \"A labeled diagram illustrating the components of a transformer model, including the encoder, decoder, self-attention mechanism, and feedforward layers.\"'"
        },
        {
        "name": "userMessage",
        "source": "='Please describe this image.'"
        },
        {
        "name": "image",
        "source": "/document/normalized_images/*/data"
        }
        ],
        "outputs": [
            {
            "name": "response",
            "targetName": "verbalizedImage"
            }
        ]
  },    
  {
      "@odata.type": "#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill",
      "name": "verbalized-image-embedding-skill",
      "description": "Embedding skill for verbalized images",
      "resourceUri": "{{textEmbeddingModelUri}}",
      "apiKey": "{{textEmbeddingModelKey}}",
      "deploymentId":"{{textEmbeddingDeploymentId}}",
      "modelName":"{{textEmbeddingModelName}}",
      "dimensions": 3072,
      "context": "/document/normalized_images/*",
      "inputs": [
         {
         "name": "text",
         "source": "/document/normalized_images/*/verbalizedImage",
         "inputs": []
         }
      ],
      "outputs": [
         {
         "name": "embedding",
         "targetName": "verbalizedImage_vector"
         }
      ]
},
    {
      "@odata.type": "#Microsoft.Skills.Util.ShaperSkill",
      "name": "shaper-skill",
      "description": "Shaper skill to reshape the data to fit the index schema",
      "context": "/document/normalized_images/*",
      "inputs": [
        {
          "name": "normalized_images",
          "source": "/document/normalized_images/*",
          "inputs": []
        },
        {
          "name": "imagePath",
          "source": "='{{imageProjectionContainer}}/'+$(/document/normalized_images/*/imagePath)",
          "inputs": []
        },
        {
          "name": "location_metadata",
          "sourceContext": "/document/normalized_images/*",
          "inputs": [
            {
              "name": "page_number",
              "source": "/document/normalized_images/*/pageNumber"
            },
            {
              "name": "bounding_polygons",
              "source": "/document/normalized_images/*/boundingPolygon"
            }              
          ]
        }        
      ],
      "outputs": [
        {
          "name": "output",
          "targetName": "new_normalized_images"
        }
      ]
   }
   ],
  "indexProjections": {
      "selectors": [
        {
          "targetIndexName": "demo-multimodal-index",
          "parentKeyFieldName": "text_document_id",
          "sourceContext": "/document/pages/*",
          "mappings": [              
            {
              "name": "content_embedding",
              "source": "/document/pages/*/text_vector"
            },
            {
              "name": "content_text",
              "source": "/document/pages/*"
            },             
            {
              "name": "document_title",
              "source": "/document/document_title"
            }      
          ]
        },
        {
          "targetIndexName": "demo-multimodal-index",
          "parentKeyFieldName": "image_document_id",
          "sourceContext": "/document/normalized_images/*",
          "mappings": [    
            {
            "name": "content_text",
            "source": "/document/normalized_images/*/verbalizedImage"
            },  
            {
            "name": "content_embedding",
            "source": "/document/normalized_images/*/verbalizedImage_vector"
            },                                           
            {
              "name": "content_path",
              "source": "/document/normalized_images/*/new_normalized_images/imagePath"
            },                    
            {
              "name": "document_title",
              "source": "/document/document_title"
            },
            {
              "name": "location_metadata",
              "source": "/document/normalized_images/*/new_normalized_images/location_metadata"
            }            
          ]
        }
      ],
      "parameters": {
        "projectionMode": "skipIndexingParentDocuments"
      }
  },
   "knowledgeStore":{
      "storageConnectionString":"{{storageConnection}}",
      "identity":null,
      "projections":[
         {
            "files":[
               {
                  "storageContainer":"{{imageProjectionContainer}}",
                  "source":"/document/normalized_images/*"
               }
            ]
         }
      ]
   }
}

### Create and run an indexer
POST {{searchUrl}}/indexers?api-version=2025-11-01-preview   HTTP/1.1
  Content-Type: application/json
  Authorization: Bearer {{token}}

{
   "name":"demo-multimodal-indexer",
   "dataSourceName":"demo-multimodal-ds",
   "targetIndexName":"demo-multimodal-index",
   "skillsetName":"demo-multimodal-skillset",
   "parameters":{
      "maxFailedItems":-1,
      "maxFailedItemsPerBatch":0,
      "batchSize":1,
      "configuration":{
         "allowSkillsetToReadFileData":true
      }
   },
   "fieldMappings":[
      {
         "sourceFieldName":"metadata_storage_name",
         "targetFieldName":"document_title"
      }
   ],
   "outputFieldMappings":[ ]
}

### Query the index
POST {{searchUrl}}/indexes/demo-multimodal-index/docs/search?api-version=2025-11-01-preview   HTTP/1.1
  Content-Type: application/json
  Authorization: Bearer {{token}}
  
  {
    "search": "*",
    "count": true
  }


### Query for only images
POST {{searchUrl}}/indexes/demo-multimodal-index/docs/search?api-version=2025-11-01-preview   HTTP/1.1
  Content-Type: application/json
  Authorization: Bearer {{token}}
  
{
  "search": "*",
  "count": true,
  "filter": "image_document_id ne null"
}

### Query for text or images with content related to energy, returning the id, parent document, and text (only populated for text chunks), and the content path where the image is saved in the knowledge store (only populated for images)
POST {{searchUrl}}/indexes/demo-multimodal-index/docs/search?api-version=2025-11-01-preview   HTTP/1.1
  Content-Type: application/json
  Authorization: Bearer {{token}}
  

{
  "search": "energy",
  "count": true,
  "select": "content_id, document_title, content_text, content_path"
}

### Reset the indexer
POST {{searchUrl}}/indexers/demo-multimodal-indexer/reset?api-version=2025-11-01-preview   HTTP/1.1
  Content-Type: application/json
  Authorization: Bearer {{token}}

### Rerun the indexer
POST {{searchUrl}}/indexers/demo-multimodal-indexer/run?api-version=2025-11-01-preview   HTTP/1.1
Content-Type: application/json
Authorization: Bearer {{token}}

### Check indexer status 
GET {{searchUrl}}/indexers/demo-multimodal-indexer/status?api-version=2025-11-01-preview   HTTP/1.1
  Content-Type: application/json
  Authorization: Bearer {{token}}
